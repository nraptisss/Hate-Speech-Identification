{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', category=UserWarning, module='tensorflow_io')\nimport logging\nimport time\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport spacy\nfrom tqdm import tqdm\n    from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split # Removed ParameterGrid for grid search\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom imblearn.over_sampling import SMOTE\nfrom torch.optim import AdamW, RAdam\nfrom torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\nfrom torch.cuda.amp import GradScaler, autocast\nfrom transformers import BartTokenizer, BartConfig, BartForSequenceClassification, logging as hf_logging\n\nfrom datasets import load_dataset\nfrom sklearn.utils.class_weight import compute_class_weight\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-17T08:33:05.592111Z","iopub.execute_input":"2023-10-17T08:33:05.592412Z","iopub.status.idle":"2023-10-17T08:33:22.704815Z","shell.execute_reply.started":"2023-10-17T08:33:05.592388Z","shell.execute_reply":"2023-10-17T08:33:22.703809Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Configure logging level to suppress warnings from transformers library\nlogging.basicConfig(level=logging.ERROR)\nhf_logging.set_verbosity_error()\n\n# Load the small BERT tokenizer\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\nmax_length = 512\n\n# Load the Davidson and ethos dataset\ndf_davidson = pd.read_csv('https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv')\nethos_data = load_dataset('ethos', 'binary')\nethos_df = ethos_data['train'].to_pandas()\n\n# Load the HateXplain dataset from Hugging Face\nhatexplain_data = load_dataset('hatexplain')\nhatexplain_df = hatexplain_data['train'].to_pandas()\nprint(\"Davidson dataset:\")\nprint(df_davidson.head())\nprint(\"Ethos dataset:\")\nprint(ethos_df.head())\nprint(\"Hatexplain dataset:\")\nprint(hatexplain_df.head())\nnlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n\n# Load the OLID dataset\nolid_train_df = pd.read_csv('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_text.txt', sep='\\t', header=None, names=['tweet'])\nolid_train_labels = pd.read_csv('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_labels.txt', sep='\\t', header=None, names=['class'])\nolid_train_df = pd.concat([olid_train_df, olid_train_labels], axis=1)\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n\ndef preprocess_text(text):\n    \"\"\"\n    Preprocess text data by removing URLs, mentions, hashtags,\n    and punctuation, and applying lemmatization.\n    \"\"\"\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove URLs, mentions, and hashtags\n    text = re.sub(r'http\\S+', '', text)\n    text = re.sub(r'@\\w+', '', text)\n    text = re.sub(r'#\\w+', '', text)\n\n    # Remove punctuation\n    text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n\n    # Apply lemmatization\n    doc = nlp(text)\n    words = [token.lemma_ for token in doc]\n\n    # Remove stopwords\n    words = [word for word in words if word not in stop_words]\n    \n    # Join the words back into a string\n    text = ' '.join(words)\n\n    return text\n\n# Combine the 'Hate Speech' and 'Offensive Language' classes in Davidson dataset by taking majority vote among annotators for each post.\nlabel_map = {0: 0, 1: 0, 2: 1}\ndf_davidson['class'] = df_davidson['class'].map(label_map)\n\n# Preprocess the text data using the preprocess_text function in Davidson dataset.\ndf_davidson['tweet'] = df_davidson['tweet'].apply(preprocess_text)\n\n# Preprocess the text data using the preprocess_text function in Ethos dataset.\nethos_df['text'] = ethos_df['text'].apply(preprocess_text)\n\n# Preprocess the HateXplain dataset using the preprocess_text function in HateXplain dataset.\nhatexplain_df['post_tokens'] = hatexplain_df['post_tokens'].apply(lambda x: ' '.join(x)).apply(preprocess_text)\n\n# Preprocess the text data using the preprocess_text function in OLID dataset.\nolid_train_df['tweet'] = olid_train_df['tweet'].apply(preprocess_text)\n\n# Combine hate speech and offensive speech labels in HateXplain dataset by taking majority vote among annotators for each post.\nhatexplain_df['label'] = hatexplain_df['annotators'].apply(lambda x: int(sum(x['label']) / len(x['label']) >= 1))\n\n# Rename columns in Ethos and HateXplain datasets\nethos_df.rename(columns={'text': 'tweet', 'label': 'class'}, inplace=True)\nhatexplain_df.rename(columns={'post_tokens': 'tweet', 'label': 'class'}, inplace=True)\n\n# Combine the Davidson, Ethos and HateXplain datasets.\ndf_combined = pd.concat([\n    df_davidson[['tweet', 'class']],\n    ethos_df[['tweet', 'class']],\n    hatexplain_df[['tweet', 'class']]\n], axis=0)\n\n# Combine the OLID dataset with the existing training data\ndf_combined = pd.concat([df_combined, olid_train_df], axis=0)\n\n# Drop rows with missing values.\ndf_combined.dropna(subset=['class'], inplace=True)\n\n# Print the shape of the combined dataset\nprint(f'Combined dataset shape: {df_combined.shape}')\n\n# Print the number of samples in each class\nprint('Number of samples in each class:')\nprint(df_combined['class'].value_counts())\n\n# Print the first few rows of the combined dataset\nprint('First few rows of the combined dataset:')\nprint(df_combined.head())\n\n# Print the number of missing values in each column\nprint('Number of missing values in each column:')\nprint(df_combined.isnull().sum())\n\n# Print the length of the longest text in the combined dataset\nprint('Length of the longest text:')\nprint(df_combined['tweet'].str.len().max())\n\n# Print the average length of the text in the combined dataset\nprint('Average length of the text:')\nprint(df_combined['tweet'].str.len().mean())\n\ntotal_rows = df_davidson.shape[0] + ethos_df.shape[0] + hatexplain_df.shape[0] + olid_train_df.shape[0]\nif df_combined.shape[0] == total_rows:\n    print('All data is included in the final dataset.')\nelse:\n    print('Some data is missing from the final dataset.')\n\n# Split the combined dataset into train and validation sets.\ntrain_df, val_df = train_test_split(df_combined, test_size=0.2, random_state=42, stratify=df_combined['class'])\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(train_df['class']), y=train_df['class'])\nclass_weights = torch.tensor(class_weights).to(device)\n\n# Tokenize and pad the text data using the BERT tokenizer.\ntrain_inputs = tokenizer.batch_encode_plus(\n    train_df['tweet'].tolist(),\n    max_length=max_length,\n    padding='max_length',\n    truncation=True,\n    return_tensors='pt',\n    batch_size=32\n)\ntrain_labels = pd.get_dummies(train_df['class']).values\n\n# Apply SMOTE to balance the class distribution in the training data\nsmote = SMOTE(random_state=42)\ntrain_inputs_smote, train_labels_smote = smote.fit_resample(train_inputs['input_ids'], train_labels)\n\n# Convert the training data to tensors\ntrain_inputs_smote = torch.tensor(train_inputs_smote)\ntrain_labels_smote = torch.tensor(train_labels_smote)\n\nval_inputs = tokenizer.batch_encode_plus(\n    val_df['tweet'].tolist(),\n    max_length=max_length,\n    padding='max_length',\n    truncation=True,\n    return_tensors='pt',\n    batch_size=32\n)\nval_labels = torch.tensor(pd.get_dummies(val_df['class']).values)\n# Convert train_labels and val_labels from numpy arrays to PyTorch tensors\ntrain_labels = torch.tensor(train_labels)\nval_labels = torch.tensor(val_labels)\n\n# Define the BERT configuration with dropout\nconfig = BartConfig.from_pretrained('facebook/bart-base', num_labels=2, hidden_dropout=0.5, attention_dropout=0.5)\n\n# Define the BERT model for sequence classification with dropout\nmodel = BartForSequenceClassification.from_pretrained('facebook/bart-base', config=config)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:33:32.243696Z","iopub.execute_input":"2023-10-17T08:33:32.244387Z","iopub.status.idle":"2023-10-17T08:37:35.247190Z","shell.execute_reply.started":"2023-10-17T08:33:32.244356Z","shell.execute_reply":"2023-10-17T08:37:35.246077Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93a1f88d87bf411780018257a5a772e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef425dc52a147c99d20c134b6eceb97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"262a40043c104e74875953bdf3a3b444"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae43f43d4634c829112e982be796af7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/940 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f171e01bf3f4e44b2fd11613afaaa80"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset ethos/binary (download: 121.01 KiB, generated: 121.90 KiB, post-processed: Unknown size, total: 242.91 KiB) to /root/.cache/huggingface/datasets/ethos/binary/1.0.0/898d3d005459ee3ff80dbeec2f169c6b7ea13de31a08458193e27dec3dd9ae38...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd29e4cde752405e8a13246f520c8729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/50.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7a94009cfca4719a9f815bc9268430a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72de15b63d8a4f69a39830bcebcb93a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/998 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset ethos downloaded and prepared to /root/.cache/huggingface/datasets/ethos/binary/1.0.0/898d3d005459ee3ff80dbeec2f169c6b7ea13de31a08458193e27dec3dd9ae38. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0acbea7abc234f94ac5cb8471bd33d50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf10704479e141fca731cd25ffb34c89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ec544bd209c41a187f4469100ff64be"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset hatexplain/plain_text (download: 12.25 MiB, generated: 8.47 MiB, post-processed: Unknown size, total: 20.73 MiB) to /root/.cache/huggingface/datasets/hatexplain/plain_text/1.0.0/df474d8d8667d89ef30649bf66e9c856ad8305bef4bc147e8e31cbdf1b8e0249...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dff6ff8c4cfb4c9693bb5c71c478b495"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe3530d1dff450a823e65234abc4bbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/145k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b7ba1686ca6456083c3d847cc741249"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33ee8b16e5904929ae321b13b0ae1126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/15383 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1922 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1924 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset hatexplain downloaded and prepared to /root/.cache/huggingface/datasets/hatexplain/plain_text/1.0.0/df474d8d8667d89ef30649bf66e9c856ad8305bef4bc147e8e31cbdf1b8e0249. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e848accd99444c3d8f22475134a249f6"}},"metadata":{}},{"name":"stdout","text":"Davidson dataset:\n   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n0           0      3            0                   0        3      2   \n1           1      3            0                   3        0      1   \n2           2      3            0                   3        0      1   \n3           3      3            0                   2        1      1   \n4           4      6            0                   6        0      1   \n\n                                               tweet  \n0  !!! RT @mayasolovely: As a woman you shouldn't...  \n1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \nEthos dataset:\n                                                text  label\n0          You should know women's sports are a joke      1\n1    You look like Sloth with deeper Down’s syndrome      1\n2  You look like Russian and speak like Indian. B...      1\n3               Women deserve to be abused, I guess.      1\n4  Women are made for making babies and cooking d...      1\nHatexplain dataset:\n                            id  \\\n0                 23107796_gab   \n1                  9995600_gab   \n2  1227920812235051008_twitter   \n3  1204931715778543624_twitter   \n4  1179102559241244672_twitter   \n\n                                          annotators  \\\n0  {'label': [0, 2, 2], 'annotator_id': [203, 204...   \n1  {'label': [2, 2, 0], 'annotator_id': [27, 6, 4...   \n2  {'label': [2, 2, 2], 'annotator_id': [209, 203...   \n3  {'label': [2, 2, 2], 'annotator_id': [235, 222...   \n4  {'label': [2, 2, 2], 'annotator_id': [51, 25, ...   \n\n                                          rationales  \\\n0  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,...   \n1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,...   \n3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, ...   \n4  [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,...   \n\n                                         post_tokens  \n0  [u, really, think, i, would, not, have, been, ...  \n1  [the, uk, has, threatened, to, return, radioac...  \n2  [if, english, is, not, imposition, then, hindi...  \n3  [no, liberal, congratulated, hindu, refugees, ...  \n4  [he, said, bro, even, your, texts, sound, redn...  \n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nCombined dataset shape: (53080, 2)\nNumber of samples in each class:\n0    34415\n1    18665\nName: class, dtype: int64\nFirst few rows of the combined dataset:\n                                               tweet  class\n0    rt   woman complain clean house amp man alwa...      1\n1    rt   boy dat coldtyga dwn bad cuffin dat hoe...      0\n2    rt   dawg rt   ever fuck bitch start cry con...      0\n3                             rt    look like tranny      0\n4    rt   shit hear I might true might faker bitc...      0\nNumber of missing values in each column:\ntweet    0\nclass    0\ndtype: int64\nLength of the longest text:\n1975\nAverage length of the text:\n60.87946495855313\nAll data is included in the final dataset.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28/2579165390.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  val_labels = torch.tensor(val_labels)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89f583dff01f430c950f97016555ded6"}},"metadata":{}}]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:39:29.730473Z","iopub.execute_input":"2023-10-17T08:39:29.730853Z","iopub.status.idle":"2023-10-17T08:39:30.101588Z","shell.execute_reply.started":"2023-10-17T08:39:29.730806Z","shell.execute_reply":"2023-10-17T08:39:30.100233Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Check if GPU is available.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\ntrain_inputs.to(device)\ntrain_labels.to(device)\nval_inputs.to(device)\nval_labels.to(device)\n\nnum_epochs = 10 # Added number of epochs hyperparameter\n\nimport optuna\nfrom optuna.pruners import MedianPruner\nfrom optuna.samplers import BaseSampler\n\nbatch_size = 16\n\nclass FixedSampler(BaseSampler):\n    def __init__(self, fixed_params):\n        self.fixed_params = fixed_params\n        self.index = 0\n\n    def infer_relative_search_space(\n        self,\n        study,\n        trial\n    ):\n        return {}\n\n    def sample_relative(\n        self,\n        study,\n        trial,\n        search_space\n    ):\n        return {}\n\n    def sample_independent(self, study, trial, param_name, param_distribution):\n        value = self.fixed_params[param_name][self.index]\n        self.index += 1\n        return value\n\ndef objective(trial):\n    # Define the hyperparameters to tune\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-4)\n    # Define the optimizer with learning rate # Modified for grid search\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)\n    scaler = GradScaler()\n\n    # Define the training hyperparameters\n    accumulation_steps = 4\n    max_grad_norm = 1.0\n    val_loss = torch.tensor(0.0)\n\n    # Define the early stopping hyperparameters\n    patience = 2\n    best_val_loss = float('inf')\n    counter = 0\n\n    # Compute class weights to use with weighted cross-entropy loss\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['class']), y=train_df['class'])\n    class_weights = torch.tensor(class_weights).to(device)\n\n    # Create an empty list to accumulate gradients\n    grad_accumulator = []\n\n    # Create a DataLoader for the training data with batch size # Modified for grid search\n    train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        # Create a progress bar for the current epoch\n        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}')\n\n        for step, batch in enumerate(progress_bar):\n            # Forward pass\n            input_ids = batch[0].to(device)\n            attention_mask = batch[1].to(device)\n            labels = batch[2].to(device).float()\n\n            with autocast():\n                criterion = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            # Backward pass\n            scaler.scale(loss).backward()\n\n            # Accumulate gradients\n            if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n                # Unscale the gradients\n                scaler.unscale_(optimizer)\n\n                # Clip the gradients to prevent exploding gradients\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n                # Accumulate the unscaled gradients\n                grad_accumulator.append(loss.item())\n\n                # Update the model parameters\n                scaler.step(optimizer)\n\n                # Clear the gradients\n                scaler.update()\n                model.zero_grad()\n\n                # Print the average loss over accumulation steps\n                avg_loss = sum(grad_accumulator) / len(grad_accumulator)\n                progress_bar.set_postfix({'Loss': avg_loss})\n\n                # Clear the gradient accumulator\n                grad_accumulator = []\n            else:\n                # Accumulate the unscaled gradients\n                grad_accumulator.append(loss.item())\n\n            total_loss += loss.item()\n\n        average_loss = total_loss / len(train_dataloader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}\")\n\n        # Evaluate the model on the validation set\n        model.eval()\n\n        # Create a DataLoader for the validation data with batch size \n        val_dataset = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels)\n        val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n\n        val_predictions = []\n        val_total_loss = 0.0\n\n        for batch in val_dataloader:\n            input_ids = batch[0].to(device)\n            attention_mask = batch[1].to(device)\n            labels = batch[2].to(device).float()\n\n            with torch.no_grad():\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                predictions = outputs.logits.argmax(dim=1).cpu().numpy()\n                val_predictions.extend(predictions)\n                val_total_loss += outputs.loss.item()\n\n        val_predictions = np.array(val_predictions)\n        val_accuracy = (val_predictions == val_df['class'].values).mean()\n        val_classification_report = classification_report(val_df['class'].values, val_predictions)\n\n        print(f'Validation Accuracy: {val_accuracy:.3f}')\n        print('Classification Report:')\n        print(val_classification_report)\n\n    return val_total_loss / len(val_dataloader)\n\n# Define the hyperparameters and their fixed values\nfixed_params = {'learning_rate': [1e-5, 1e-4]}\n\n# Create an Optuna study with the TPESampler and MedianPruner\nstudy = optuna.create_study(\n    direction='minimize',\n    sampler=FixedSampler(fixed_params),\n    pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=5)\n)\n\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Optimize the objective function\nstudy.optimize(objective, n_trials=len(fixed_params['learning_rate']))\n\n# Print the best hyperparameters\nprint(study.best_params)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T08:39:32.001045Z","iopub.execute_input":"2023-10-17T08:39:32.002429Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"[I 2023-10-17 08:39:32,502] A new study created in memory with name: no-name-40af457a-3bc2-458b-8118-ce35a8ca4d98\nEpoch 1: 100%|██████████| 2654/2654 [31:41<00:00,  1.40it/s, Loss=0.513]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Average Loss: 0.6477\nValidation Accuracy: 0.780\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.80      0.89      0.84      6883\n           1       0.74      0.58      0.65      3733\n\n    accuracy                           0.78     10616\n   macro avg       0.77      0.74      0.75     10616\nweighted avg       0.78      0.78      0.77     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 2654/2654 [31:45<00:00,  1.39it/s, Loss=0.715]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Average Loss: 0.5345\nValidation Accuracy: 0.793\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.80      0.90      0.85      6883\n           1       0.76      0.59      0.67      3733\n\n    accuracy                           0.79     10616\n   macro avg       0.78      0.75      0.76     10616\nweighted avg       0.79      0.79      0.79     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 2654/2654 [31:45<00:00,  1.39it/s, Loss=0.671]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Average Loss: 0.4975\nValidation Accuracy: 0.813\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.84      0.87      0.86      6883\n           1       0.75      0.70      0.73      3733\n\n    accuracy                           0.81     10616\n   macro avg       0.80      0.79      0.79     10616\nweighted avg       0.81      0.81      0.81     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 2654/2654 [31:45<00:00,  1.39it/s, Loss=0.453]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Average Loss: 0.4745\nValidation Accuracy: 0.817\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.86      0.86      0.86      6883\n           1       0.74      0.73      0.74      3733\n\n    accuracy                           0.82     10616\n   macro avg       0.80      0.80      0.80     10616\nweighted avg       0.82      0.82      0.82     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 2654/2654 [31:45<00:00,  1.39it/s, Loss=0.673]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Average Loss: 0.4569\nValidation Accuracy: 0.821\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.85      0.87      0.86      6883\n           1       0.75      0.73      0.74      3733\n\n    accuracy                           0.82     10616\n   macro avg       0.80      0.80      0.80     10616\nweighted avg       0.82      0.82      0.82     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 2654/2654 [31:46<00:00,  1.39it/s, Loss=0.381]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10], Average Loss: 0.4438\nValidation Accuracy: 0.830\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.86      0.88      0.87      6883\n           1       0.77      0.74      0.75      3733\n\n    accuracy                           0.83     10616\n   macro avg       0.82      0.81      0.81     10616\nweighted avg       0.83      0.83      0.83     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 2654/2654 [31:45<00:00,  1.39it/s, Loss=0.644]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Average Loss: 0.4322\nValidation Accuracy: 0.828\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.85      0.89      0.87      6883\n           1       0.78      0.71      0.74      3733\n\n    accuracy                           0.83     10616\n   macro avg       0.81      0.80      0.81     10616\nweighted avg       0.83      0.83      0.83     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 2654/2654 [31:45<00:00,  1.39it/s, Loss=0.382]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Average Loss: 0.4156\nValidation Accuracy: 0.834\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.86      0.90      0.88      6883\n           1       0.79      0.72      0.75      3733\n\n    accuracy                           0.83     10616\n   macro avg       0.82      0.81      0.81     10616\nweighted avg       0.83      0.83      0.83     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 2654/2654 [31:44<00:00,  1.39it/s, Loss=0.415]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10], Average Loss: 0.4054\nValidation Accuracy: 0.833\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.87      0.87      0.87      6883\n           1       0.76      0.77      0.76      3733\n\n    accuracy                           0.83     10616\n   macro avg       0.82      0.82      0.82     10616\nweighted avg       0.83      0.83      0.83     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 2654/2654 [31:44<00:00,  1.39it/s, Loss=0.624]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10], Average Loss: 0.3957\n","output_type":"stream"},{"name":"stderr","text":"[I 2023-10-17 15:10:14,182] Trial 0 finished with value: 0.4165781802688557 and parameters: {'learning_rate': 1e-05}. Best is trial 0 with value: 0.4165781802688557.\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.828\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.84      0.90      0.87      6883\n           1       0.80      0.69      0.74      3733\n\n    accuracy                           0.83     10616\n   macro avg       0.82      0.80      0.80     10616\nweighted avg       0.83      0.83      0.82     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 2654/2654 [31:44<00:00,  1.39it/s, Loss=0.574]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Average Loss: 0.4769\nValidation Accuracy: 0.830\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.87      0.86      0.87      6883\n           1       0.75      0.77      0.76      3733\n\n    accuracy                           0.83     10616\n   macro avg       0.81      0.82      0.81     10616\nweighted avg       0.83      0.83      0.83     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 2654/2654 [31:45<00:00,  1.39it/s, Loss=0.469]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Average Loss: 0.4264\nValidation Accuracy: 0.823\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.85      0.88      0.86      6883\n           1       0.76      0.72      0.74      3733\n\n    accuracy                           0.82     10616\n   macro avg       0.81      0.80      0.80     10616\nweighted avg       0.82      0.82      0.82     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 2654/2654 [31:44<00:00,  1.39it/s, Loss=0.178]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Average Loss: 0.3845\nValidation Accuracy: 0.829\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.85      0.89      0.87      6883\n           1       0.78      0.71      0.75      3733\n\n    accuracy                           0.83     10616\n   macro avg       0.82      0.80      0.81     10616\nweighted avg       0.83      0.83      0.83     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 2654/2654 [31:46<00:00,  1.39it/s, Loss=0.446]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Average Loss: 0.3310\nValidation Accuracy: 0.812\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.84      0.87      0.86      6883\n           1       0.75      0.71      0.73      3733\n\n    accuracy                           0.81     10616\n   macro avg       0.80      0.79      0.79     10616\nweighted avg       0.81      0.81      0.81     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 2654/2654 [31:45<00:00,  1.39it/s, Loss=0.172] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Average Loss: 0.2807\nValidation Accuracy: 0.806\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.82      0.91      0.86      6883\n           1       0.78      0.62      0.69      3733\n\n    accuracy                           0.81     10616\n   macro avg       0.80      0.76      0.78     10616\nweighted avg       0.80      0.81      0.80     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 2654/2654 [31:46<00:00,  1.39it/s, Loss=0.321] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10], Average Loss: 0.2393\nValidation Accuracy: 0.808\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.84      0.87      0.86      6883\n           1       0.74      0.69      0.72      3733\n\n    accuracy                           0.81     10616\n   macro avg       0.79      0.78      0.79     10616\nweighted avg       0.81      0.81      0.81     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 2654/2654 [31:45<00:00,  1.39it/s, Loss=0.283] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Average Loss: 0.2120\nValidation Accuracy: 0.811\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.86      0.85      0.85      6883\n           1       0.73      0.73      0.73      3733\n\n    accuracy                           0.81     10616\n   macro avg       0.79      0.79      0.79     10616\nweighted avg       0.81      0.81      0.81     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 2654/2654 [31:43<00:00,  1.39it/s, Loss=0.414] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Average Loss: 0.1939\nValidation Accuracy: 0.805\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.84      0.86      0.85      6883\n           1       0.73      0.70      0.71      3733\n\n    accuracy                           0.80     10616\n   macro avg       0.79      0.78      0.78     10616\nweighted avg       0.80      0.80      0.80     10616\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9:  32%|███▏      | 855/2654 [10:13<21:16,  1.41it/s, Loss=0.265] ","output_type":"stream"}]}]}